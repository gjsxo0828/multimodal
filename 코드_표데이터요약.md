# PDFì—ì„œ í‘œ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ì—¬ ìš”ì•½í•˜ê¸°

## ì‹¤ìŠµ ëª©í‘œ
---
`Unstructured` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ PDF íŒŒì¼ì—ì„œ í‘œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ ìš”ì•½í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

Note. `Unstructured` ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” PDF íŒŒì‹±ì— ì•„ë˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•©ë‹ˆë‹¤: 

* `tesseract`: Optical Character Recognition (OCR)
* `poppler`: PDF ë Œë”ë§

## ì‹¤ìŠµ ëª©ì°¨
---
1. **í™˜ê²½ ê²€ì¦**: ì‹¤ìŠµì„ ìœ„í•´ ì‚¬ì „ì— ì¤€ë¹„ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì˜ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
2. **PDF íŒŒì¼ ë¡œë“œ**: PDF íŒŒì¼ì„ ë¡œë“œí•˜ê³ , í‘œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
3. **Document ë¼ë²¨ ìƒì„±**: Retrieverê°€ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•  ë•Œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì„¤ëª… ë¼ë²¨ì„ ìƒì„±í•©ë‹ˆë‹¤.
4. **ê°„ë‹¨í•œ RAG êµ¬í˜„**: ìƒì„±í•œ Document ë¼ë²¨ì„ í™œìš©í•˜ì—¬ ê°„ë‹¨í•œ RAGë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

## 1. í™˜ê²½ ê²€ì¦

ì‹¤ìŠµì„ ìœ„í•´ ì‚¬ì „ì— ì¤€ë¹„ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ëª¨ë‘ ì˜ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.


```python
import uuid
from typing import Any

from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.stores import InMemoryStore
from langchain_ollama import ChatOllama, OllamaEmbeddings
from pydantic import BaseModel
from unstructured.partition.pdf import partition_pdf
```

    /home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
      from .autonotebook import tqdm as notebook_tqdm


### 1.1. Ollamaë¥¼ í™œìš©í•œ ë¡œì»¬ LLM ëª¨ë¸ í™•ì¸

Ollamaë¥¼ í†µí•´ llama 3.1 8B ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.


```python
!ollama pull llama3.1
```

    [?25lpulling manifest â ‹ [?25h[?25l[2K[1Gpulling manifest â ™ [?25h[?25l[2K[1Gpulling manifest â ¹ [?25h[?25l[2K[1Gpulling manifest â ¸ [?25h[?25l[2K[1Gpulling manifest â ¼ [?25h[?25l[2K[1Gpulling manifest â ´ [?25h[?25l[2K[1Gpulling manifest â ¦ [?25h[?25l[2K[1Gpulling manifest â § [?25h[?25l[2K[1Gpulling manifest â ‡ [?25h[?25l[2K[1Gpulling manifest â  [?25h[?25l[2K[1Gpulling manifest â ‹ [?25h[?25l[2K[1Gpulling manifest â ™ [?25h[?25l[2K[1Gpulling manifest â ¹ [?25h[?25l[2K[1Gpulling manifest â ¸ [?25h[?25l[2K[1Gpulling manifest â ¼ [?25h[?25l[2K[1Gpulling manifest â ´ [?25h[?25l[2K[1Gpulling manifest â ¦ [?25h[?25l[2K[1Gpulling manifest â § [?25h[?25l[2K[1Gpulling manifest [?25h
    Error: open /mnt/elice/dataset/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29-partial-0: read-only file system



```python
llm = ChatOllama(model="llama3.1", temperature=0)
embeddings = OllamaEmbeddings(model="llama3.1")
```

### 1.2. Unstructured ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ê¸° ìœ„í•œ Dependency ë¬´ê²°ì„± í™•ì¸


```python
# PDF íŒŒì‹± ê³¼ì •ì—ì„œ GPUë¥¼ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ë¯¸ë¦¬ ì„¤ì¹˜í•œ CUDA, cuDNN ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ê²½ë¡œë¥¼
# í™˜ê²½ë³€ìˆ˜ LD_LIBRARY_PATHì— ì¶”ê°€í•©ë‹ˆë‹¤.
import os
os.environ['LD_LIBRARY_PATH'] = "/usr/local/cuda/lib64:" + os.environ['LD_LIBRARY_PATH']
```


```python
import tensorrt
from onnxruntime.capi import _pybind_state as C

# OCR ê¸°ëŠ¥ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ TensorRT ë° ONNXRT ë²„ì „ì„ í™•ì¸í•©ë‹ˆë‹¤.
# Ref. https://docs.nvidia.com/deeplearning/tensorrt/latest/installing-tensorrt/installing.html#python-package-index-installation
print(tensorrt.__version__)
assert tensorrt.Builder(tensorrt.Logger())

# Ref. https://github.com/Unstructured-IO/unstructured/issues/2506
# ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'] ê°€ ì¶œë ¥ë˜ë©´ ë©ë‹ˆë‹¤.
print(f"Available ONNXRT providers: {C.get_available_providers()}")
```

    10.8.0.43
    Available ONNXRT providers: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']


## 2. PDF íŒŒì¼ ë¡œë“œ

ì‹¤ìŠµì—ì„œ ì‚¬ìš©í•˜ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ LLM ëª¨ë¸ì¸ `llama3.1` ëª¨ë¸ì„ ì„¤ëª…í•˜ëŠ” ë…¼ë¬¸ì˜ ì¼ë¶€ (1~3ì±•í„°) ë¥¼ ë¶ˆëŸ¬ì˜¤ê³ , ì„±ëŠ¥ ë¹„êµë¥¼ ìœ„í•´ ì‚¬ìš©í•˜ëŠ” í‘œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê² ìŠµë‹ˆë‹¤.

ë…¼ë¬¸ ì „ë¬¸ì€ ë‹¤ìŒ ë§í¬ë¥¼ í†µí•´ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤: https://ai.meta.com/research/publications/the-llama-3-herd-of-models/


```python
data_dir = "./data"
```

`Unstructured` ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `partition_pdf`ë¥¼ í™œìš©í•´ PDF íŒŒì¼ì—ì„œ í‘œ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨, ë³„ë„ì˜ ë©”íƒ€ ë°ì´í„° ë“±ì„ í™œìš©í•´ ë¶„ë¦¬í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë¼ì„œ 100% ë¶„ë¦¬í•  ìˆ˜ ìˆë‹¤ê³  ë³´ì¥í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤.

ì…ë ¥í•œ PDF ë¬¸ì„œëŠ” 15í˜ì´ì§€ë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë©°, ì²˜ìŒ ë¶„í• í•  ë•ŒëŠ” ì´ˆê¸° ì„¸íŒ… ê³¼ì •ì„ í¬í•¨í•˜ì—¬ ì•½ 5ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤.


```python
%%time
# from unstructured.partition.pdf import partition_pdf
raw_pdf_elements = partition_pdf(   
    filename=os.path.join(data_dir, "Llama3.1_Ch1~3.pdf"),
    extract_images_in_pdf=False, #ì´ë¯¸ì§€ ì¶”ì¶œì—¬ë¶€
    infer_table_structure=True, # í‘œ ì¶”ì¶œ ì—¬ë¶€
    strategy="auto", #PDFë¥¼ ë¶„í• í•˜ê³  ì¶”ì¶œí•˜ëŠ” ì „ëµì„ ì§€ì •
    # "auto" : ë¬¸ì„œ íŠ¹ì„±ê³¼ ë‹¤ë¥¸ íŒŒë¼ë¯¸í…Œì–´ ë”°ë¼ ìë™ìœ¼ë¡œ ìµœì ì˜ ì „ëµì„ ì„ íƒ
    # 'hi_res" : ë¬¸ì„œ ë ˆì´ì•„ì›ƒ ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ê³ ì •ë°€ ì¶”ì¶œ, í‘œ/ì´ë¯¸ì§€ ì¸ì‹ì— ê°•ì 
    # "ocr_only" : ì´ë¯¸ì§€ ê¸°ë°˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    # "fast" : í…ìŠ¤íŠ¸ ì¶”ì¶œì´ ë¹ ë¥¸ ì „ëµ, í…ìŠ¤íŠ¸ê°€ ì˜ ì¶”ì¶œë˜ëŠ” PDF ì í•©
    image_output_dir_path=data_dir # ì¶”ì¶œëœ ì´ë¯¸ì§€ ì €ì¥ ê²½ë¡œ
)
```

    CPU times: user 1min 4s, sys: 10.3 s, total: 1min 14s
    Wall time: 3min 26s



```python
len(raw_pdf_elements)
```




    211



ë¶„í• ì´ ì™„ë£Œë˜ì—ˆìœ¼ë©´ ë¶„í• ëœ Chunk ì¢…ë¥˜ë¥¼ í™•ì¸í•´ë´…ì‹œë‹¤.


```python
# Create a dictionary to store counts of each type
category_counts = {}

for element in raw_pdf_elements:
    category = str(type(element)) #ìë£Œí˜• strë¡œ
    if category in category_counts: # key ê°’ì´ ìˆìœ¼ë©´
        category_counts[category] += 1 #value + 1
    else:
        category_counts[category] = 1

# Unique_categories will have unique elements
unique_categories = set(category_counts.keys())
category_counts
```




    {"<class 'unstructured.documents.elements.Text'>": 21,
     "<class 'unstructured.documents.elements.Header'>": 1,
     "<class 'unstructured.documents.elements.Title'>": 32,
     "<class 'unstructured.documents.elements.Image'>": 8,
     "<class 'unstructured.documents.elements.NarrativeText'>": 103,
     "<class 'unstructured.documents.elements.ListItem'>": 33,
     "<class 'unstructured.documents.elements.Table'>": 5,
     "<class 'unstructured.documents.elements.Footer'>": 3,
     "<class 'unstructured.documents.elements.FigureCaption'>": 4,
     "<class 'unstructured.documents.elements.Formula'>": 1}



`Table`ì„ í¬í•¨í•˜ì—¬ ì—¬ëŸ¬ ì¢…ë¥˜ì˜ Elementë¡œ ë¶„í• ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì €í¬ëŠ” ì´ ì¤‘ì—ì„œ `NarrativeText`, `ListItem` ìš”ì†Œë¥¼ í…ìŠ¤íŠ¸ë¡œ, `Table`ì„ í‘œ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ë²„ë¦¬ê² ìŠµë‹ˆë‹¤.


```python
#pydantic : type hintingì„ í†µí•´ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬
#pydanticì˜ BaseModel : ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ë“±ì˜ í•¨ìˆ˜ ë° ë³€ìˆ˜ ì§€ì›í•˜ëŠ” base class
class Element(BaseModel):
    type: str #type í•„ë“œëŠ” str ìë£Œí˜•ì„ ê°€ì ¸ì•¼ í•¨
    text: Any #ì–´ë–¤ ê²ƒë„ ê°€ëŠ¥
#-> ì´ ë°ì´í„° êµ¬ì¡°ë¥¼ í†µí•´ì„œ ê° elementì˜ ìš”ì†Œë¥¼ ì œì–´

new_chunk_after_n = 3200
cur_chunk_text = ""

# Categorize by type
categorized_elements = []
for element in raw_pdf_elements:
    # unstructured.documents.elements.Table
    if "Table" in str(type(element)):
        categorized_elements.append(Element(type="table", text=str(element)))
        if len(cur_chunk_text) > 0:
            categorized_elements.append(Element(type="text", text=cur_chunk_text))
            cur_chunk_text = ""
    # unstructured.documents.elements.NarrativeText, unstructured.documents.elements.ListItem
    elif "NarrativeText" in str(type(element)) or "ListItem" in str(type(element)):
        cur_chunk_text += str(element)
        if len(cur_chunk_text) > new_chunk_after_n:
            categorized_elements.append(Element(type="text", text=cur_chunk_text))
            cur_chunk_text = ""

if len(cur_chunk_text) > 0:
    categorized_elements.append(Element(type="text", text=cur_chunk_text))
# Tables
table_elements = [e for e in categorized_elements if e.type == "table"]

# Text
text_elements = [e for e in categorized_elements if e.type == "text"]
```

## 3. Document ë¼ë²¨ ìƒì„±

RAG ì‹œìŠ¤í…œì´ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•  ë•Œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì„¤ëª… ë¼ë²¨ì„ ìƒì„±í•©ë‹ˆë‹¤.

ê°„ë‹¨í•œ Chainì„ êµ¬ì„±í•˜ì—¬ í‘œ ë°ì´í„°ë¥¼ í¬í•¨í•œ ê° Chunk ë³„ ì„¤ëª…ë¬¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.


```python
# Note. ìš”ì•½ë¬¸ì„ ìƒì„±í•  ë•Œ ì‚¬ìš©í•œ llama3.1 8B ëª¨ë¸ì€ í•œêµ­ì–´ í”„ë¡¬í”„íŠ¸ë¡œ ìš”ì•½ì„ ìš”ì²­í•  ê²½ìš° ìš”ì•½ í€„ë¦¬í‹°ê°€ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
prompt_text = """You are an assistant tasked with summarizing tables and text. \
Give a concise summary of the table or text. Table or text chunk: {element} """
prompt = ChatPromptTemplate.from_template(prompt_text)

# Summary chain
summarize_chain = {"element": lambda x: x} | prompt | llm | StrOutputParser()
```

ê° í‘œì— ëŒ€í•œ ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.


```python
# Apply to tables
tables = [i.text for i in table_elements]
table_summaries = summarize_chain.batch(tables, {"max_concurrency": 5})
```


```python
table_summaries
```




    ['Here is a concise summary of the table:\n\nThe table lists various versions of the Llama 3 tool, with different parameters such as model size (8B, 70B, 405B) and instruction capabilities (âœ—/âœ“). The release dates range from April to July 2024.',
     'Here is a concise summary of the table:\n\nThe table presents various benchmarking results for different models and tasks, including MMLU (Mathematical Math), HumanEval, MBPP EvalPlus, GSM8K (Math), MATH, ARC Challenge, GPQA, Tool use, BFCL Nexus, ZeroSCROLLS/QuALITY, InfiniteBench/En.MC, NIH/Multi-needle, and MGSM (Multilingual).\n\nThe results show that different models perform well on various tasks, with some achieving high accuracy rates:\n\n* MMLU-Pro (5-shot) achieves 89.1% accuracy\n* HumanEval (0-shot) achieves 80.4% accuracy\n* MBPP EvalPlus (0-shot) achieves 72.6% accuracy\n* GSM8K (8-shot, CoT) achieves 95.1% accuracy\n* MATH (0-shot, CoT) achieves 96.8% accuracy\n* ARC Challenge (0-shot) achieves 83.4% accuracy\n* GPQA (0-shot, CoT) achieves 94.8% accuracy\n* BFCL Nexus achieves 84.8% accuracy\n* ZeroSCROLLS/QuALITY achieves 95.2% accuracy\n* InfiniteBench/En.MC achieves 83.4% accuracy\n* NIH/Multi-needle achieves 100.0% accuracy\n* MGSM (0-shot, CoT) achieves 89.9% accuracy\n\nOverall, the table highlights the performance of various models on different tasks and provides a snapshot of the current state-of-the-art in these areas.',
     'Here is a concise summary of the table:\n\nThis table describes three different models with varying dimensions and hyperparameters. The models have 32, 80, or 126 layers, respectively, and correspond to model dimensions of 4,096, 8,192, or 16,384. They also differ in their fully connected neural network (FFN) dimension, attention heads, key/value heads, peak learning rate, activation function, vocabulary size, and positional embeddings.',
     'Here is a concise summary of the table:\n\nThe table compares different GPU configurations with varying parameters such as batch size, tokens per batch, and TFLOPs (tera floating-point operations) per GPU. The results show that increasing the batch size from 8 to 16 can lead to a decrease in performance, measured by TFLOPs, by around 10% (from 430 to 400).',
     'Here is a concise summary of the table:\n\nThe top causes of faults in a system are related to the GPU, with faulty GPUs and HBM3 memory being the most common issues (30.1% and 17.2%, respectively). Software bugs and network-related problems also contribute significantly to faults, while host maintenance and hardware components such as NICs, SSDs, and power supplies also experience issues.']



ì´ì–´ì„œ ëª¨ë“  ë¬¸ì„œì— ëŒ€í•œ ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ ì•½ 2ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤.


```python
# Apply to texts
texts = [i.text for i in text_elements]
text_summaries = summarize_chain.batch(texts, {"max_concurrency": 20})
```


```python
text_summaries
```




    ['Here is a concise summary:\n\nThis paper presents Llama 3, a new set of foundation models that support multilinguality, coding, reasoning, and tool usage. The largest model has 405 billion parameters and can process up to 128K tokens. Llama 3 delivers comparable quality to leading language models like GPT-4 on various tasks and is publicly released. The paper also explores integrating image, video, and speech capabilities into Llama 3 via a compositional approach, achieving competitive results with state-of-the-art models.',
     'Here is a concise summary of the text:\n\nThe Llama 3 project developed three multilingual language models (8B, 70B, and 405B parameters) that outperform smaller models trained using the same procedure. The flagship model performs on par with leading language models like GPT-4 across various tasks and matches state-of-the-art results. Smaller models are best-in-class, delivering a better balance between helpfulness and harmlessness than its predecessor. All three models are publicly released under an updated license, along with multimodal extensions for image, video, and speech recognition capabilities (still in development).',
     'Here is a concise summary:\n\nThree models (Llama 3, 8B, and 70B) were pre-trained on multiple languages, but designed primarily for use in English.',
     'Here is a concise summary:\n\n**Table Summary:**\nThe table compares the performance of three Llama 3 models (8B, 70B, and 405B) with competing models on key benchmark evaluations. The best-performing model in each size equivalence class is bolded.\n\n**Text Summary:**\nLlama 3 language models are developed through two stages:\n\n1. **Language Model Pre-training**: A large multilingual text corpus is converted to discrete tokens and pre-trained using a next-token prediction task, resulting in a rich understanding of language.\n2. **Language Model Post-training**: The pre-trained model is fine-tuned on instruction tuning data and Direct Preference Optimization to align it with human feedback, integrate new capabilities (e.g., tool-use), and incorporate safety mitigations.\n\nThe resulting models have a wide range of capabilities, including answering questions in multiple languages, writing code, solving complex reasoning problems, and using tools. Additional experiments add image, video, and speech capabilities using a compositional approach.',
     'Here is a concise summary:\n\n**Multimodal Model Training**\n\nThe text describes training multimodal models for image, video, and speech understanding. The models are trained using adapters that integrate pre-trained encoders into a finetuned language model. The adapters are trained on paired data (text-image/video) to align representations. A speech adapter is also integrated, allowing the model to support interaction via a speech interface.\n\n**Language Model Pre-training**\n\nThe text also describes the process of pre-training a large-scale language model. This involves creating a dataset from various web sources, applying de-duplication and cleaning methods, and removing personally identifiable information (PII) and adult content. The data is then processed using a custom parser to extract high-quality diverse text, which is used to train the language model.',
     'Here is a concise summary of the text:\n\nThe process involves multiple rounds of de-duplication at URL, document, and line levels, as well as heuristic filtering and model-based quality filtering to remove low-quality documents and outliers. This includes using techniques such as MinHash, duplicated n-gram coverage ratio, "dirty word" counting, and token-distribution Kullback-Leibler divergence to filter out unwanted content. Additionally, domain-specific pipelines are built to extract code and math-relevant web pages, and multilingual text processing is implemented with filters to remove PII or unsafe content.',
     'Here is a concise summary of the text:\n\nThe authors describe their approach to pre-training a language model, Llama 3, using a diverse data mix and techniques such as de-duplication, quality ranking, and annealing. They detail how they determine the optimal data mix through knowledge classification and scaling law experiments, resulting in a final mix with 50% general knowledge, 25% mathematical and reasoning tokens, 17% code tokens, and 8% multilingual tokens. The authors also demonstrate the effectiveness of annealing on small amounts of high-quality data to boost model performance and evaluate its efficacy on various benchmarks, finding significant improvements for smaller models but negligible gains for their flagship 405B model.',
     'Here is a concise summary of the text:\n\nLlama 3 uses a similar model architecture to Llama and Llama 2, with some minor modifications to improve performance. These changes include using grouped query attention and an attention mask to prevent self-attention between documents in the same sequence, which had a limited impact on standard pre-training but improved results for very long sequences.',
     'Here is a concise summary of the table and text:\n\n**Key Hyperparameters of Llama 3:**\n\n* Vocabulary with 128K tokens (combining English and non-English languages)\n* Increased RoPE base frequency to 500,000 for better context support\n* Model size optimized for compute budget of 3.8 Ã— 10^25 FLOPs\n\n**Scaling Laws:**\n\n* Developed a two-stage methodology to predict downstream benchmark performance\n* Correlated negative log-likelihood with training FLOPs and task accuracy\n* Used Llama 2 family models to select pre-training data mix and predict task performance\n* Conducted scaling law experiments with compute budgets between 6 Ã— 10^18 FLOPs and 10^22 FLOPs.',
     "Here is a concise summary of the text:\n\nThe authors describe their approach to scaling up pre-training for the Llama 3 model using a large compute budget (up to 16.55T tokens). They identify a power-law relation between compute budget and optimal number of training tokens, which they use to predict the performance of the flagship model on downstream tasks. The authors also discuss their hardware and infrastructure setup, including Meta's AI Research SuperCluster and Grand Teton AI server platform, which enabled them to train the Llama 3 405B model efficiently. They report that their two-step scaling law prediction accurately forecasted the final performance of the flagship model.",
     'Here is a concise summary of the text:\n\nThe Llama 3 model uses a high-performance network fabric based on RDMA over Converged Ethernet (RoCE) or Infiniband, with 400 Gbps interconnects between GPUs. The RoCE-based cluster has a three-layer Clos network topology with 24K GPUs connected across multiple pods and datacenter buildings, with load balancing techniques employed to minimize network communication and optimize traffic flow.',
     "Here is a concise summary of the text:\n\nThe authors describe optimizations made to train large models (up to 24K GPUs) without traditional congestion control methods. They use Enhanced-ECMP protocol for load balancing and deep-buffer switches for congestion control, achieving efficient GPU utilization (38-43% BF16 Model FLOPs Utilization). To scale training, they employ 4D parallelism, combining tensor, pipeline, context, and data parallelism to shard the model across GPUs. This approach ensures each GPU's resources fit in its HBM memory. The authors also address challenges with existing pipeline parallelism implementations, such as batch size constraints, and propose improvements to achieve better performance.",
     'Here is a concise summary of the text:\n\nThe authors address memory and computation imbalances in existing pipeline parallelism implementations by modifying their schedule to allow flexible setting of the number of micro-batches per stage (N). They reduce one Transformer layer from the first and last stages, use an interleaved schedule with asynchronous point-to-point communication, and proactively deallocate tensors not used for future computation. These optimizations enable pre-training Llama 3 on sequences of up to 8K tokens without activation checkpointing. Additionally, they utilize context parallelism (CP) to improve memory efficiency when scaling the context length, enabling training on extremely long sequences up to 128K in length.',
     'Here is a concise summary of the text:\n\nThe authors describe optimizations made to improve the performance and efficiency of distributed training on large-scale models, specifically for the Llama 3 model. They discuss:\n\n* Optimizing network-aware parallelism configuration to minimize communication overhead\n* Improving numerical stability through gradient accumulation and reduced-scatter gradients in FP32\n* Developing a memory consumption estimator and performance-projection tool to explore parallelism configurations\n* Enhancing collective communication library NCCLX for Llama 3, addressing inefficiencies such as data chunking and staged data copy.',
     "Here is a concise summary:\n\nDuring a 54-day period of pre-training for Llama 3, there were 466 job interruptions, with 78% attributed to confirmed or suspected hardware issues (mainly GPU failures). Despite these challenges, the team achieved over 90% effective training time while supporting automated cluster maintenance. To improve efficiency, they reduced startup and checkpointing times, developed tools for fast diagnosis and problem resolution, and used PyTorch's NCCL flight recorder to quickly diagnose hangs and performance issues.",
     'Here is a concise summary of the text:\n\nThe authors discuss challenges in training large-scale models like Llama 3, including hardware issues that can cause slow "stragglers" and environmental factors that affect performance. They developed tools to identify problematic communications and observed diurnal throughput variations due to temperature fluctuations. The recipe for pre-training Llama 3 consists of three stages: initial pre-training, long-context pre-training, and annealing. The authors also describe adjustments made to the data mix during training to improve model performance on specific downstream tasks, including increasing non-English data, upsampled mathematical data, and downsampling lower-quality subsets.',
     "Here is a concise summary:\n\nThe post-training approach for Llama 3 involves three main steps: rejection sampling, supervised fine-tuning, and direct preference optimization. A reward model is first trained on human-annotated data, followed by fine-tuning the pre-trained checkpoint with SFT and further alignment with DPO. This process aims to improve the model's capabilities in areas such as reasoning, coding, factuality, multilingual understanding, tool use, long context, and precise instruction following."]



## 4. ê°„ë‹¨í•œ RAG êµ¬í˜„

### 4.1. VectorDB êµ¬ì„±

ìƒì„±í•œ Document ë¼ë²¨ì„ í™œìš©í•˜ì—¬ ê°„ë‹¨í•œ RAGë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

- `InMemoryStore`ë¥¼ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë° í‘œ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤
- `vectorstore`ì—ëŠ” ì „ì²´ ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ëŠ” ëŒ€ì‹ , ìš”ì•½ëœ í…ìŠ¤íŠ¸ë§Œ ì„ë² ë”©í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.


```python
vectorstore = Chroma(
    embedding_function=embeddings, collection_name="summaries"
)

store = InMemoryStore()
id_key = "doc_id"

# VectorDBì—ì„œ ë°”ë¡œ Retrieverë¥¼ ìƒì„±í•˜ëŠ” ëŒ€ì‹ , ë³„ë„ì˜ Retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    docstore=store,
    id_key=id_key,
)
```


```python
# Add texts
doc_ids = [str(uuid.uuid4()) for _ in texts]
summary_texts = [
    Document(page_content=s, metadata={id_key: doc_ids[i]})
    for i, s in enumerate(text_summaries)
]
retriever.vectorstore.add_documents(summary_texts)
retriever.docstore.mset(list(zip(doc_ids, texts)))

# Add tables
table_ids = [str(uuid.uuid4()) for _ in tables]
summary_tables = [
    Document(page_content=s, metadata={id_key: table_ids[i]})
    for i, s in enumerate(table_summaries)
]
retriever.vectorstore.add_documents(summary_tables)
retriever.docstore.mset(list(zip(table_ids, tables)))
```

### 4.2. RAG êµ¬ì„±

ê°„ë‹¨í•œ RAG Chainì„ êµ¬ì„±í•˜ê³ , ì˜ë„ëŒ€ë¡œ ì˜ ì‘ë™í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

Note. ì•„ë˜ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•œ ëª¨ë¸ì˜ í•œê³„ë¡œ ì¸í•´, ì˜ì–´ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì•¼ ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


```python
template = """Answer the question based only on the following context, which can include text and tables:
{context}
Question: {question}
"""
# RAG pipeline
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | ChatPromptTemplate.from_template(template)
    | llm
    | StrOutputParser()
)
```


```python
print(chain.invoke("Show me the performance of the Llama 3"))
```

    Unfortunately, there is no direct answer to this question in the provided context. However, I can try to extract some relevant information related to the performance of Llama 3.
    
    According to Table 2, which presents the performance of finetuned Llama 3 models on key benchmark evaluations, we see that:
    
    * The 8B version of Llama 3 performs similarly to GPT-4 (OpenAI, 2023a) across various tasks.
    * The 70B and 405B versions of Llama 3 outperform alternative models with similar numbers of parameters (Bai et al., 2023; Jiang et al., 2023).
    * The smaller models (8B and 70B) are best-in-class, while the larger model (405B) is close to matching the state-of-the-art.
    
    Additionally, it's mentioned that Llama 3 delivers a much better balance between helpfulness and harmlessness than its predecessor (Touvron et al., 2023b).
    
    Please note that this information is not directly answering your question but rather providing some context about the performance of Llama 3. If you'd like to know more specific details, I'll be happy to try and help!


ìœ„ ì˜ˆì‹œ ì§ˆë¬¸ì€ ì•„ë˜ í‘œì— ëŒ€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤.

![image](https://github.com/user-attachments/assets/b9c739ec-1eb5-4e8f-9840-2a37f55a5eba)

