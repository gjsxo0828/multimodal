# PDF에서 표 데이터를 분리하여 요약하기

## 실습 목표
---
`Unstructured` 라이브러리를 활용하여 PDF 파일에서 표 데이터를 추출하고, 이를 요약하는 방법을 학습합니다.

Note. `Unstructured` 라이브러리는 PDF 파싱에 아래 라이브러리를 활용합니다: 

* `tesseract`: Optical Character Recognition (OCR)
* `poppler`: PDF 렌더링

## 실습 목차
---
1. **환경 검증**: 실습을 위해 사전에 준비된 라이브러리가 모두 잘 설치되어 있는지 확인합니다.
2. **PDF 파일 로드**: PDF 파일을 로드하고, 표 데이터를 추출합니다.
3. **Document 라벨 생성**: Retriever가 문서를 검색할 때 활용할 수 있는 설명 라벨을 생성합니다.
4. **간단한 RAG 구현**: 생성한 Document 라벨을 활용하여 간단한 RAG를 구현합니다.

## 1. 환경 검증

실습을 위해 사전에 준비된 라이브러리가 모두 잘 설치되어 있는지 확인합니다.


```python
import uuid
from typing import Any

from langchain.retrievers.multi_vector import MultiVectorRetriever
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.stores import InMemoryStore
from langchain_ollama import ChatOllama, OllamaEmbeddings
from pydantic import BaseModel
from unstructured.partition.pdf import partition_pdf
```

    /home/elicer/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
      from .autonotebook import tqdm as notebook_tqdm


### 1.1. Ollama를 활용한 로컬 LLM 모델 확인

Ollama를 통해 llama 3.1 8B 모델을 불러옵니다.


```python
!ollama pull llama3.1
```

    [?25lpulling manifest ⠋ [?25h[?25l[2K[1Gpulling manifest ⠙ [?25h[?25l[2K[1Gpulling manifest ⠹ [?25h[?25l[2K[1Gpulling manifest ⠸ [?25h[?25l[2K[1Gpulling manifest ⠼ [?25h[?25l[2K[1Gpulling manifest ⠴ [?25h[?25l[2K[1Gpulling manifest ⠦ [?25h[?25l[2K[1Gpulling manifest ⠧ [?25h[?25l[2K[1Gpulling manifest ⠇ [?25h[?25l[2K[1Gpulling manifest ⠏ [?25h[?25l[2K[1Gpulling manifest ⠋ [?25h[?25l[2K[1Gpulling manifest ⠙ [?25h[?25l[2K[1Gpulling manifest ⠹ [?25h[?25l[2K[1Gpulling manifest ⠸ [?25h[?25l[2K[1Gpulling manifest ⠼ [?25h[?25l[2K[1Gpulling manifest ⠴ [?25h[?25l[2K[1Gpulling manifest ⠦ [?25h[?25l[2K[1Gpulling manifest ⠧ [?25h[?25l[2K[1Gpulling manifest [?25h
    Error: open /mnt/elice/dataset/models/blobs/sha256-667b0c1932bc6ffc593ed1d03f895bf2dc8dc6df21db3042284a6f4416b06a29-partial-0: read-only file system



```python
llm = ChatOllama(model="llama3.1", temperature=0)
embeddings = OllamaEmbeddings(model="llama3.1")
```

### 1.2. Unstructured 라이브러리를 활용하기 위한 Dependency 무결성 확인


```python
# PDF 파싱 과정에서 GPU를 활용할 수 있도록 미리 설치한 CUDA, cuDNN 라이브러리의 경로를
# 환경변수 LD_LIBRARY_PATH에 추가합니다.
import os
os.environ['LD_LIBRARY_PATH'] = "/usr/local/cuda/lib64:" + os.environ['LD_LIBRARY_PATH']
```


```python
import tensorrt
from onnxruntime.capi import _pybind_state as C

# OCR 기능을 효율적으로 사용하기 위한 TensorRT 및 ONNXRT 버전을 확인합니다.
# Ref. https://docs.nvidia.com/deeplearning/tensorrt/latest/installing-tensorrt/installing.html#python-package-index-installation
print(tensorrt.__version__)
assert tensorrt.Builder(tensorrt.Logger())

# Ref. https://github.com/Unstructured-IO/unstructured/issues/2506
# ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'] 가 출력되면 됩니다.
print(f"Available ONNXRT providers: {C.get_available_providers()}")
```

    10.8.0.43
    Available ONNXRT providers: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']


## 2. PDF 파일 로드

실습에서 사용하는 오픈 소스 LLM 모델인 `llama3.1` 모델을 설명하는 논문의 일부 (1~3챕터) 를 불러오고, 성능 비교를 위해 사용하는 표 데이터를 추출하겠습니다.

논문 전문은 다음 링크를 통해 확인하실 수 있습니다: https://ai.meta.com/research/publications/the-llama-3-herd-of-models/


```python
data_dir = "./data"
```

`Unstructured` 라이브러리의 `partition_pdf`를 활용해 PDF 파일에서 표 데이터를 분리할 수 있습니다. 단, 별도의 메타 데이터 등을 활용해 분리하는 것은 아니라서 100% 분리할 수 있다고 보장하는 것은 아닙니다.

입력한 PDF 문서는 15페이지로 이루어져 있으며, 처음 분할할 때는 초기 세팅 과정을 포함하여 약 5분 정도 소요됩니다.


```python
%%time
# from unstructured.partition.pdf import partition_pdf
raw_pdf_elements = partition_pdf(   
    filename=os.path.join(data_dir, "Llama3.1_Ch1~3.pdf"),
    extract_images_in_pdf=False, #이미지 추출여부
    infer_table_structure=True, # 표 추출 여부
    strategy="auto", #PDF를 분할하고 추출하는 전략을 지정
    # "auto" : 문서 특성과 다른 파라미테어 따라 자동으로 최적의 전략을 선택
    # 'hi_res" : 문서 레이아웃 분석 기반으로 고정밀 추출, 표/이미지 인식에 강점
    # "ocr_only" : 이미지 기반 텍스트 추출
    # "fast" : 텍스트 추출이 빠른 전략, 텍스트가 잘 추출되는 PDF 적합
    image_output_dir_path=data_dir # 추출된 이미지 저장 경로
)
```

    CPU times: user 1min 4s, sys: 10.3 s, total: 1min 14s
    Wall time: 3min 26s



```python
len(raw_pdf_elements)
```




    211



분할이 완료되었으면 분할된 Chunk 종류를 확인해봅시다.


```python
# Create a dictionary to store counts of each type
category_counts = {}

for element in raw_pdf_elements:
    category = str(type(element)) #자료형 str로
    if category in category_counts: # key 값이 있으면
        category_counts[category] += 1 #value + 1
    else:
        category_counts[category] = 1

# Unique_categories will have unique elements
unique_categories = set(category_counts.keys())
category_counts
```




    {"<class 'unstructured.documents.elements.Text'>": 21,
     "<class 'unstructured.documents.elements.Header'>": 1,
     "<class 'unstructured.documents.elements.Title'>": 32,
     "<class 'unstructured.documents.elements.Image'>": 8,
     "<class 'unstructured.documents.elements.NarrativeText'>": 103,
     "<class 'unstructured.documents.elements.ListItem'>": 33,
     "<class 'unstructured.documents.elements.Table'>": 5,
     "<class 'unstructured.documents.elements.Footer'>": 3,
     "<class 'unstructured.documents.elements.FigureCaption'>": 4,
     "<class 'unstructured.documents.elements.Formula'>": 1}



`Table`을 포함하여 여러 종류의 Element로 분할된 것을 확인할 수 있습니다. 저희는 이 중에서 `NarrativeText`, `ListItem` 요소를 텍스트로, `Table`을 표 데이터로 사용하고, 나머지는 버리겠습니다.


```python
#pydantic : type hinting을 통해 데이터 유효성 검사
#pydantic의 BaseModel : 데이터 유효성 검사 등의 함수 및 변수 지원하는 base class
class Element(BaseModel):
    type: str #type 필드는 str 자료형을 가져야 함
    text: Any #어떤 것도 가능
#-> 이 데이터 구조를 통해서 각 element의 요소를 제어

new_chunk_after_n = 3200
cur_chunk_text = ""

# Categorize by type
categorized_elements = []
for element in raw_pdf_elements:
    # unstructured.documents.elements.Table
    if "Table" in str(type(element)):
        categorized_elements.append(Element(type="table", text=str(element)))
        if len(cur_chunk_text) > 0:
            categorized_elements.append(Element(type="text", text=cur_chunk_text))
            cur_chunk_text = ""
    # unstructured.documents.elements.NarrativeText, unstructured.documents.elements.ListItem
    elif "NarrativeText" in str(type(element)) or "ListItem" in str(type(element)):
        cur_chunk_text += str(element)
        if len(cur_chunk_text) > new_chunk_after_n:
            categorized_elements.append(Element(type="text", text=cur_chunk_text))
            cur_chunk_text = ""

if len(cur_chunk_text) > 0:
    categorized_elements.append(Element(type="text", text=cur_chunk_text))
# Tables
table_elements = [e for e in categorized_elements if e.type == "table"]

# Text
text_elements = [e for e in categorized_elements if e.type == "text"]
```

## 3. Document 라벨 생성

RAG 시스템이 문서를 검색할 때 활용할 수 있는 설명 라벨을 생성합니다.

간단한 Chain을 구성하여 표 데이터를 포함한 각 Chunk 별 설명문을 구성합니다.


```python
# Note. 요약문을 생성할 때 사용한 llama3.1 8B 모델은 한국어 프롬프트로 요약을 요청할 경우 요약 퀄리티가 떨어질 수 있습니다.
prompt_text = """You are an assistant tasked with summarizing tables and text. \
Give a concise summary of the table or text. Table or text chunk: {element} """
prompt = ChatPromptTemplate.from_template(prompt_text)

# Summary chain
summarize_chain = {"element": lambda x: x} | prompt | llm | StrOutputParser()
```

각 표에 대한 설명을 생성합니다.


```python
# Apply to tables
tables = [i.text for i in table_elements]
table_summaries = summarize_chain.batch(tables, {"max_concurrency": 5})
```


```python
table_summaries
```




    ['Here is a concise summary of the table:\n\nThe table lists various versions of the Llama 3 tool, with different parameters such as model size (8B, 70B, 405B) and instruction capabilities (✗/✓). The release dates range from April to July 2024.',
     'Here is a concise summary of the table:\n\nThe table presents various benchmarking results for different models and tasks, including MMLU (Mathematical Math), HumanEval, MBPP EvalPlus, GSM8K (Math), MATH, ARC Challenge, GPQA, Tool use, BFCL Nexus, ZeroSCROLLS/QuALITY, InfiniteBench/En.MC, NIH/Multi-needle, and MGSM (Multilingual).\n\nThe results show that different models perform well on various tasks, with some achieving high accuracy rates:\n\n* MMLU-Pro (5-shot) achieves 89.1% accuracy\n* HumanEval (0-shot) achieves 80.4% accuracy\n* MBPP EvalPlus (0-shot) achieves 72.6% accuracy\n* GSM8K (8-shot, CoT) achieves 95.1% accuracy\n* MATH (0-shot, CoT) achieves 96.8% accuracy\n* ARC Challenge (0-shot) achieves 83.4% accuracy\n* GPQA (0-shot, CoT) achieves 94.8% accuracy\n* BFCL Nexus achieves 84.8% accuracy\n* ZeroSCROLLS/QuALITY achieves 95.2% accuracy\n* InfiniteBench/En.MC achieves 83.4% accuracy\n* NIH/Multi-needle achieves 100.0% accuracy\n* MGSM (0-shot, CoT) achieves 89.9% accuracy\n\nOverall, the table highlights the performance of various models on different tasks and provides a snapshot of the current state-of-the-art in these areas.',
     'Here is a concise summary of the table:\n\nThis table describes three different models with varying dimensions and hyperparameters. The models have 32, 80, or 126 layers, respectively, and correspond to model dimensions of 4,096, 8,192, or 16,384. They also differ in their fully connected neural network (FFN) dimension, attention heads, key/value heads, peak learning rate, activation function, vocabulary size, and positional embeddings.',
     'Here is a concise summary of the table:\n\nThe table compares different GPU configurations with varying parameters such as batch size, tokens per batch, and TFLOPs (tera floating-point operations) per GPU. The results show that increasing the batch size from 8 to 16 can lead to a decrease in performance, measured by TFLOPs, by around 10% (from 430 to 400).',
     'Here is a concise summary of the table:\n\nThe top causes of faults in a system are related to the GPU, with faulty GPUs and HBM3 memory being the most common issues (30.1% and 17.2%, respectively). Software bugs and network-related problems also contribute significantly to faults, while host maintenance and hardware components such as NICs, SSDs, and power supplies also experience issues.']



이어서 모든 문서에 대한 설명을 생성합니다. 이 과정은 약 2분 정도 소요됩니다.


```python
# Apply to texts
texts = [i.text for i in text_elements]
text_summaries = summarize_chain.batch(texts, {"max_concurrency": 20})
```


```python
text_summaries
```




    ['Here is a concise summary:\n\nThis paper presents Llama 3, a new set of foundation models that support multilinguality, coding, reasoning, and tool usage. The largest model has 405 billion parameters and can process up to 128K tokens. Llama 3 delivers comparable quality to leading language models like GPT-4 on various tasks and is publicly released. The paper also explores integrating image, video, and speech capabilities into Llama 3 via a compositional approach, achieving competitive results with state-of-the-art models.',
     'Here is a concise summary of the text:\n\nThe Llama 3 project developed three multilingual language models (8B, 70B, and 405B parameters) that outperform smaller models trained using the same procedure. The flagship model performs on par with leading language models like GPT-4 across various tasks and matches state-of-the-art results. Smaller models are best-in-class, delivering a better balance between helpfulness and harmlessness than its predecessor. All three models are publicly released under an updated license, along with multimodal extensions for image, video, and speech recognition capabilities (still in development).',
     'Here is a concise summary:\n\nThree models (Llama 3, 8B, and 70B) were pre-trained on multiple languages, but designed primarily for use in English.',
     'Here is a concise summary:\n\n**Table Summary:**\nThe table compares the performance of three Llama 3 models (8B, 70B, and 405B) with competing models on key benchmark evaluations. The best-performing model in each size equivalence class is bolded.\n\n**Text Summary:**\nLlama 3 language models are developed through two stages:\n\n1. **Language Model Pre-training**: A large multilingual text corpus is converted to discrete tokens and pre-trained using a next-token prediction task, resulting in a rich understanding of language.\n2. **Language Model Post-training**: The pre-trained model is fine-tuned on instruction tuning data and Direct Preference Optimization to align it with human feedback, integrate new capabilities (e.g., tool-use), and incorporate safety mitigations.\n\nThe resulting models have a wide range of capabilities, including answering questions in multiple languages, writing code, solving complex reasoning problems, and using tools. Additional experiments add image, video, and speech capabilities using a compositional approach.',
     'Here is a concise summary:\n\n**Multimodal Model Training**\n\nThe text describes training multimodal models for image, video, and speech understanding. The models are trained using adapters that integrate pre-trained encoders into a finetuned language model. The adapters are trained on paired data (text-image/video) to align representations. A speech adapter is also integrated, allowing the model to support interaction via a speech interface.\n\n**Language Model Pre-training**\n\nThe text also describes the process of pre-training a large-scale language model. This involves creating a dataset from various web sources, applying de-duplication and cleaning methods, and removing personally identifiable information (PII) and adult content. The data is then processed using a custom parser to extract high-quality diverse text, which is used to train the language model.',
     'Here is a concise summary of the text:\n\nThe process involves multiple rounds of de-duplication at URL, document, and line levels, as well as heuristic filtering and model-based quality filtering to remove low-quality documents and outliers. This includes using techniques such as MinHash, duplicated n-gram coverage ratio, "dirty word" counting, and token-distribution Kullback-Leibler divergence to filter out unwanted content. Additionally, domain-specific pipelines are built to extract code and math-relevant web pages, and multilingual text processing is implemented with filters to remove PII or unsafe content.',
     'Here is a concise summary of the text:\n\nThe authors describe their approach to pre-training a language model, Llama 3, using a diverse data mix and techniques such as de-duplication, quality ranking, and annealing. They detail how they determine the optimal data mix through knowledge classification and scaling law experiments, resulting in a final mix with 50% general knowledge, 25% mathematical and reasoning tokens, 17% code tokens, and 8% multilingual tokens. The authors also demonstrate the effectiveness of annealing on small amounts of high-quality data to boost model performance and evaluate its efficacy on various benchmarks, finding significant improvements for smaller models but negligible gains for their flagship 405B model.',
     'Here is a concise summary of the text:\n\nLlama 3 uses a similar model architecture to Llama and Llama 2, with some minor modifications to improve performance. These changes include using grouped query attention and an attention mask to prevent self-attention between documents in the same sequence, which had a limited impact on standard pre-training but improved results for very long sequences.',
     'Here is a concise summary of the table and text:\n\n**Key Hyperparameters of Llama 3:**\n\n* Vocabulary with 128K tokens (combining English and non-English languages)\n* Increased RoPE base frequency to 500,000 for better context support\n* Model size optimized for compute budget of 3.8 × 10^25 FLOPs\n\n**Scaling Laws:**\n\n* Developed a two-stage methodology to predict downstream benchmark performance\n* Correlated negative log-likelihood with training FLOPs and task accuracy\n* Used Llama 2 family models to select pre-training data mix and predict task performance\n* Conducted scaling law experiments with compute budgets between 6 × 10^18 FLOPs and 10^22 FLOPs.',
     "Here is a concise summary of the text:\n\nThe authors describe their approach to scaling up pre-training for the Llama 3 model using a large compute budget (up to 16.55T tokens). They identify a power-law relation between compute budget and optimal number of training tokens, which they use to predict the performance of the flagship model on downstream tasks. The authors also discuss their hardware and infrastructure setup, including Meta's AI Research SuperCluster and Grand Teton AI server platform, which enabled them to train the Llama 3 405B model efficiently. They report that their two-step scaling law prediction accurately forecasted the final performance of the flagship model.",
     'Here is a concise summary of the text:\n\nThe Llama 3 model uses a high-performance network fabric based on RDMA over Converged Ethernet (RoCE) or Infiniband, with 400 Gbps interconnects between GPUs. The RoCE-based cluster has a three-layer Clos network topology with 24K GPUs connected across multiple pods and datacenter buildings, with load balancing techniques employed to minimize network communication and optimize traffic flow.',
     "Here is a concise summary of the text:\n\nThe authors describe optimizations made to train large models (up to 24K GPUs) without traditional congestion control methods. They use Enhanced-ECMP protocol for load balancing and deep-buffer switches for congestion control, achieving efficient GPU utilization (38-43% BF16 Model FLOPs Utilization). To scale training, they employ 4D parallelism, combining tensor, pipeline, context, and data parallelism to shard the model across GPUs. This approach ensures each GPU's resources fit in its HBM memory. The authors also address challenges with existing pipeline parallelism implementations, such as batch size constraints, and propose improvements to achieve better performance.",
     'Here is a concise summary of the text:\n\nThe authors address memory and computation imbalances in existing pipeline parallelism implementations by modifying their schedule to allow flexible setting of the number of micro-batches per stage (N). They reduce one Transformer layer from the first and last stages, use an interleaved schedule with asynchronous point-to-point communication, and proactively deallocate tensors not used for future computation. These optimizations enable pre-training Llama 3 on sequences of up to 8K tokens without activation checkpointing. Additionally, they utilize context parallelism (CP) to improve memory efficiency when scaling the context length, enabling training on extremely long sequences up to 128K in length.',
     'Here is a concise summary of the text:\n\nThe authors describe optimizations made to improve the performance and efficiency of distributed training on large-scale models, specifically for the Llama 3 model. They discuss:\n\n* Optimizing network-aware parallelism configuration to minimize communication overhead\n* Improving numerical stability through gradient accumulation and reduced-scatter gradients in FP32\n* Developing a memory consumption estimator and performance-projection tool to explore parallelism configurations\n* Enhancing collective communication library NCCLX for Llama 3, addressing inefficiencies such as data chunking and staged data copy.',
     "Here is a concise summary:\n\nDuring a 54-day period of pre-training for Llama 3, there were 466 job interruptions, with 78% attributed to confirmed or suspected hardware issues (mainly GPU failures). Despite these challenges, the team achieved over 90% effective training time while supporting automated cluster maintenance. To improve efficiency, they reduced startup and checkpointing times, developed tools for fast diagnosis and problem resolution, and used PyTorch's NCCL flight recorder to quickly diagnose hangs and performance issues.",
     'Here is a concise summary of the text:\n\nThe authors discuss challenges in training large-scale models like Llama 3, including hardware issues that can cause slow "stragglers" and environmental factors that affect performance. They developed tools to identify problematic communications and observed diurnal throughput variations due to temperature fluctuations. The recipe for pre-training Llama 3 consists of three stages: initial pre-training, long-context pre-training, and annealing. The authors also describe adjustments made to the data mix during training to improve model performance on specific downstream tasks, including increasing non-English data, upsampled mathematical data, and downsampling lower-quality subsets.',
     "Here is a concise summary:\n\nThe post-training approach for Llama 3 involves three main steps: rejection sampling, supervised fine-tuning, and direct preference optimization. A reward model is first trained on human-annotated data, followed by fine-tuning the pre-trained checkpoint with SFT and further alignment with DPO. This process aims to improve the model's capabilities in areas such as reasoning, coding, factuality, multilingual understanding, tool use, long context, and precise instruction following."]



## 4. 간단한 RAG 구현

### 4.1. VectorDB 구성

생성한 Document 라벨을 활용하여 간단한 RAG를 구현합니다.

- `InMemoryStore`를 활용하여 텍스트 및 표 데이터를 저장합니다
- `vectorstore`에는 전체 문서를 임베딩하는 대신, 요약된 텍스트만 임베딩하여 저장합니다.


```python
vectorstore = Chroma(
    embedding_function=embeddings, collection_name="summaries"
)

store = InMemoryStore()
id_key = "doc_id"

# VectorDB에서 바로 Retriever를 생성하는 대신, 별도의 Retriever를 생성합니다.
retriever = MultiVectorRetriever(
    vectorstore=vectorstore,
    docstore=store,
    id_key=id_key,
)
```


```python
# Add texts
doc_ids = [str(uuid.uuid4()) for _ in texts]
summary_texts = [
    Document(page_content=s, metadata={id_key: doc_ids[i]})
    for i, s in enumerate(text_summaries)
]
retriever.vectorstore.add_documents(summary_texts)
retriever.docstore.mset(list(zip(doc_ids, texts)))

# Add tables
table_ids = [str(uuid.uuid4()) for _ in tables]
summary_tables = [
    Document(page_content=s, metadata={id_key: table_ids[i]})
    for i, s in enumerate(table_summaries)
]
retriever.vectorstore.add_documents(summary_tables)
retriever.docstore.mset(list(zip(table_ids, tables)))
```

### 4.2. RAG 구성

간단한 RAG Chain을 구성하고, 의도대로 잘 작동하는지 테스트합니다.

Note. 아래 모델에서 사용한 모델의 한계로 인해, 영어로 프롬프트를 입력해야 정확한 결과를 얻을 수 있습니다.


```python
template = """Answer the question based only on the following context, which can include text and tables:
{context}
Question: {question}
"""
# RAG pipeline
chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | ChatPromptTemplate.from_template(template)
    | llm
    | StrOutputParser()
)
```


```python
print(chain.invoke("Show me the performance of the Llama 3"))
```

    Unfortunately, there is no direct answer to this question in the provided context. However, I can try to extract some relevant information related to the performance of Llama 3.
    
    According to Table 2, which presents the performance of finetuned Llama 3 models on key benchmark evaluations, we see that:
    
    * The 8B version of Llama 3 performs similarly to GPT-4 (OpenAI, 2023a) across various tasks.
    * The 70B and 405B versions of Llama 3 outperform alternative models with similar numbers of parameters (Bai et al., 2023; Jiang et al., 2023).
    * The smaller models (8B and 70B) are best-in-class, while the larger model (405B) is close to matching the state-of-the-art.
    
    Additionally, it's mentioned that Llama 3 delivers a much better balance between helpfulness and harmlessness than its predecessor (Touvron et al., 2023b).
    
    Please note that this information is not directly answering your question but rather providing some context about the performance of Llama 3. If you'd like to know more specific details, I'll be happy to try and help!


위 예시 질문은 아래 표에 대한 질문입니다.

![image](https://github.com/user-attachments/assets/b9c739ec-1eb5-4e8f-9840-2a37f55a5eba)

